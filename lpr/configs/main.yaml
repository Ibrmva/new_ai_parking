defaults:
  - _self_
  - model: parseq
  - charset: label

model:
  _convert_: all
  img_size: [32, 128]
  max_label_length: 25

  charset_train: "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"
  charset_test: "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"
  batch_size: 64
  weight_decay: 0.0
  warmup_pct: 0.075

data:
  _target_: strhub.data.module.SceneTextDataModule
  root_dir: /Users/gulaiymibraimova/ai_app/lpr/app/datasets/data_csv_lmdb
  train_dir: train
  val_dir: val
  test_dir: test
  batch_size: ${model.batch_size}
  img_size: ${model.img_size}
  charset_train: ${model.charset_train}
  charset_test: ${model.charset_test}
  max_label_length: ${model.max_label_length}
  remove_whitespace: true
  normalize_unicode: true
  augment: true         
  num_workers: 2 

trainer:
  _target_: pytorch_lightning.Trainer
  _convert_: all
  val_check_interval: 1
  log_every_n_steps: 1
  max_epochs: 60
  gradient_clip_val: 20
  accelerator: mps
  devices: 1
  check_val_every_n_epoch: 1

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_acc
    mode: max
    save_top_k: 1
    filename: "best-{epoch:02d}-{val_acc:.4f}"
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val_acc
    patience: 5
    mode: max

# pretrained: 'parseq'      
# ckpt_path: null
pretrained: false
ckpt_path: lpr/models/lpr/evaluated_ocr_model.pt

hydra:
  output_subdir: config
  run:
    dir: outputs/my_lpr_run
  sweep:
    dir: multirun/${model.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: ${hydra.job.override_dirname}
